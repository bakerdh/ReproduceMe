---
title: "Computational Reproducibility Cookbook"
author: "Daniel H. Baker, University of York"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    toc: no
    keep_tex: yes
  word_document: default
  pdf_document:
    toc: no
  html_document: default
bibliography: references.bib
csl: elife.csl
---

```{r setup, include=FALSE}

packagelist <- c('knitr','remotes','tictoc','R.matlab','osfr','grDevices','ez','BayesFactor') # list of CRAN packages

# find which packages are missing and install them
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}

# then activate all the packages with the library function
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))


knitr::opts_chunk$set(echo = TRUE)
if (!dir.exists('local/')){dir.create('local')}
if (!dir.exists('Figures/')){dir.create('Figures')}

# added to facilitate running on mybinder.org:
if(tinytex::is_tinytex()==FALSE){tinytex::install_tinytex()}
 
```

# Intro

This document contains notes describing a process for making scientific papers computationally reproducible. It is written in R markdown, and intended to serve as a handbook for the 'ReproduceMe' pilot project at the University of York in 2023. Most of the examples here involve R, as we anticipate using R for most projects, however many of the same things can be achieved in other languages.

## Background on reproducibility

The goal of computational reproducibility is that all of the analyses in a paper can be reconstructed. For modelling and simulation papers, this requires sharing of code. For empirical papers it requires sharing of both code and data. Although data sharing has become commonplace in recent years, researchers appear to be much less willing to share analysis code. This could be for any number of reasons, such as fear that they have made an error, or lack of confidence in their own coding skills. It could also be simply be that current norms in most fields do not require code sharing. However making one's work reproducible has numerous benefits, including increasing the transparency of the analysis, and the confidence of readers, reviewers and editors. Other researchers can then re-use parts of an analysis pipeline in their own work, speeding up scientific progress. Finally, it is potentially the case that a reproducible workflow benefits the authors themselves if they wish to revisit or reconstruct their own analysis in the future.

# Five levels of computational reproducibility

Computational reproducibility can be as simple as posting a script and data online. However there are additional steps that can make things easier for the end user, integrate the analysis code with the manuscript and figure generation, and preserve the computational environment used (e.g. package versions). A useful framework is the *reproducibility pyramid* illustrated in Figure \@ref(fig:repropyramid). The relative widths of each layer of the pyramid indicate how common each level is, though note the proportions are not to scale - the vast majority of current research is not reproducible at all. Some further comments on each level follow.

```{r repropyramid, fig.cap="The reproducibility pyramid, indicating five levels of computational reproducibility.", fig.align="center", echo=FALSE}

colfunc <- colorRampPalette(c("grey90", "cornflowerblue"))
colvect <- colfunc(5)

cexlist <- c(1.5,1.2)
plotlims <- c(0,1.8,0,1)
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   

polygon(c(0,1,0.5,0),c(0,0,1,0),col=colvect[1])

polygon(c(0.1,0.9,0.5,0.1),c(0.2,0.2,1,0.2),col=colvect[2])

polygon(c(0.2,0.8,0.5,0.2),c(0.4,0.4,1,0.4),col=colvect[3])

polygon(c(0.3,0.7,0.5,0.3),c(0.6,0.6,1,0.6),col=colvect[4])

polygon(c(0.4,0.6,0.5,0.4),c(0.8,0.8,1,0.8),col=colvect[5])

text(0.5,0.1,'0',adj=0.5,cex=cexlist[1])
text(0.5,0.3,'1',adj=0.5,cex=cexlist[1])
text(0.5,0.5,'2',adj=0.5,cex=cexlist[1])
text(0.5,0.7,'3',adj=0.5,cex=cexlist[1])
text(0.5,0.9,'4',adj=0.5,cex=cexlist[1])

text(1.3,0.1,'Not reproducible',adj=0.5,cex=cexlist[2])
text(1.3,0.3,'Manual download',adj=0.5,cex=cexlist[2])
text(1.3,0.5,'Automatic download',adj=0.5,cex=cexlist[2])
text(1.3,0.7,'Full analysis pipeline',adj=0.5,cex=cexlist[2])
text(1.3,0.9,'Reproducible environment',adj=0.5,cex=cexlist[2])

```

**Level 0** - the study is not computationally reproducible, usually because code and/or data are unavailable. This is the case for essentially all published research from the 20th century, as well as the vast majority of work published today. Note that non-reproducible work is not necessarily of a lower quality than reproducible work, it is just that this is harder to evaluate: we must trust that the authors' account of their analysis is full and accurate. Many high profile cases of research fraud might not have been possible, or would have been caught sooner, had reproducibility been the norm, or a requirement for publication.

**Level 1** - a single script conducts the analysis using local raw or preprocessed data that is manually downloaded. This is the most basic form of computational reproducibility. However it usually requires quite a lot of effort on the part of the end user. For example there may be many files that need to be downloaded and stored in specific places on the computer, or the code might need modifying to locate the local copies of the data files. In some cases it could also be necessary for the end user to separately download and/or install additional packages or code repositories in order for the code to work. Finally, the output is likely to be the raw function output, e.g. for a statistical test, and it may require effort and expertise to find the values reported in the paper.

**Level 2** - a single script automatically downloads and analyses raw or preprocessed data, and produces a formatted output containing values that can be incorporated into a paper. This is a more user-friendly solution, because the end user only needs to download a single script, and then all downloading of data is automated. There are several ways to do this, but the OSF provide an R package that makes it straightforward. At level 2, we anticipate that the output of any analysis is provided in a user-friendly format, such that the values included in a Results section are apparent from the output of the script.

**Level 3** - a single script automatically downloads and analyses all raw data, generates all tables and figures, and produces a pdf of the entire manuscript (note that this script may execute other scripts, e.g. in different programming languages). This is a more impressive solution, as it is clear precisely where all of the values reported in the paper have come from, and how the figures were constructed. It can be substantially more work to get to this stage, but if the paper is already written then converting to an executable format is largely procedural. The journal [eLife](https://elifesciences.org/) piloted something along these lines a few years ago, though it seems to have been forgotten about now.

**Level 4** - all code is embedded in a Docker container (or similar) that includes the software required to run (e.g. specific package versions, and versions of the programming environment). This would be extremely technically challenging to do from scratch. However fortunately there are some useful tools already available. The [Rocker](https://rocker-project.org/) project provides standard Docker containers for R studio, which can be downloaded and used as a wrapper for the entire computational environment [see @Peikert2021]. A less extreme version is to use the [*renv*](https://rstudio.github.io/renv/articles/renv.html) package to manage package versions in R (more on all this below).

Discussions about which level to aim for should be had with the study authors before work begins on making materials reproducible. It is always easier to program something when the goal is clear, and there may be idiosyncracies specific to an individual study that means that Level 3 or 4 is not practical. Sometimes there are also restrictions on sharing of raw data (e.g. where this could potentially be used to identify a participant), and in such cases we might aim for reproducibility based on preprocessed or de-identified data. All of this is totally fine - for this pilot project our goal is simply to make things more reproducible than Level 0, so we need to be pragmatic rather than puritanical.

# Implementation

## *R Markdown* is really good

[Markdown](https://www.markdownguide.org/) is a generic convention for document formatting. R Markdown takes this basic concept and integrates it into the R environment. This means you can produce a single script that contains both text and computer code. When the script is executed the code runs too, meaning that analyses can be conducted, and an output document is generated. It is possible to automate all parts of an analysis pipeline in this way, with different sections of the code importing and processing the data, generating figures, and formatting the output in the style of a paper. You can then 'knit' the markdown file to a variety of formats including pdf, Word documents, html and epub. Similar systems exist for other programming languages, including [Jupyter notebooks](https://jupyter.org/) for Python, and [Matlab live scripts](https://uk.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script-or-function.html). We now have several examples of full papers written in R Markdown, available at the following repositories (these are clickable hyperlinks):

[Baker (2021)](https://github.com/bakerdh/FourierStats)

[Baker et al., (2021)](https://osf.io/e62wu/)

[Segala et al. (2023)](https://doi.org/10.17605/OSF.IO/TBEMA)

[Baker et al. (2023)](https://doi.org/10.17605/OSF.IO/AB3YV)

In each case, the Rmd file contains the markdown script that will auto-generate everything in the paper. It's worth having a look through some of these to see how they are structured, though I expect (and hope!) that most of the papers we work on will be rather less complicated.

## Markdown file structure

There is no set structure for Markdown files, which can interleave chunks of text, code, tables and images in any order. However it is sensible to follow some basic design principles to make documents easier to navigate. In principle we could include all of the R code at the start of the file, and all of the text for the manuscript below it. However this sometimes makes it quite difficult to find different sections of code. Instead it is better to break the analysis up into several distinct chunks of code, and scatter these throughout the manuscript in appropriate locations. The caveat here is that if we wish to embed values from some analysis, or figures that have been automatically generated, these things need to happen before we try to use the results in the text. In RStudio, section headings (created using either a single or double hash symbol) and code chunk names allow the user to navigate easily through the document via the menu in the lower left hand corner of the script window.

A chunk of R code is initiated with three reverse ticks (I can't get these to render properly in the markdown system, but it's ASCII code 96), followed by the letter r (and optionally a name for the code chunk) in curly brackets, and terminated with three more reverse ticks. Here is an example:

```{r basicchunk}

a <- 10.3

```

(see the raw markdown document for the actual syntax). By default the code will be reproduced in the output document. Often we wish to hide this, which we can do with the option `include=FALSE`, added inside the curly brackets. In between the opening and closing tick lines, we can include any R code we wish, loading in data and performing analyses. Any variables we create will be stored in a sandboxed Environment and are available to subsequent code chunks in the same markdown script. We can also save results to external files, such as RData files.

It is also possible to include pieces of 'inline' code as part of a markdown document. This allows us to report the outcomes of statistical tests automatically in a Results section, for example, which helps avoid typos from manual transcription. We include an R variable using a single reverse tick followed by an r. Then we type the variable we wish to display, and it appears in the text, for example we can insert the value we assigned to the variable 'a' earlier, which was: `r a` (again see the markdown script for the syntax).

## Flags to specify the level of analysis

Some analysis pipelines can become very complicated, requiring substantial storage space, and taking a long time to execute. In such cases I have found it helpful to set a flag (normally called *processdata*) at the start of the script to control the level of detail that the analysis is performed at. The flag is hierarchical, in that setting it to a value of 2 implies that the operations from levels 0 and 1 will also be executed. Whilst this will vary across studies, one possible convention is as follows:

`processdata <- 0`: This means that no data are processed, and the manuscript is compiled using data that have already been analysed. The appropriate files are downloaded from the OSF (or other repository) if they are not available locally. This mode is particularly useful when writing a manuscript in markdown, as one can see the typeset text rapidly without waiting for analyses to execute.

`processdata <- 1`: This mode auto-generates any figures using pre-processed data. Again, any data that is required can be automatically downloaded. Once the figures have been created, the manuscript is knitted to the requested format.

`processdata <- 2`: Here we conduct 'second level' analyses that do not take a long time, for example statistical testing, using processed data and/or model outputs. These operations might require processed data files for individual participants, such as participant level averages of the dependent variable(s). An example for EEG might be the participant's average ERP waveforms for each trial.

`processdata <- 3`: The highest value for the flag specifies that all data should be downloaded and analysed from the lowest level available. Ideally this will be the raw data files recorded during an experiment. At this level of analysis we also perform any time-consuming analysis procedures, such as model fitting, bootstrapping, and so on. The outputs of all of these analyses are stored in an intermediate data file that allows the lower values of *processdata* to skip the resource-intensive analysis steps. This data file should also be stored in the project's online repository (i.e. OSF) so that it can be downloaded directly if the user does not have the resources available for the full analysis.

Different sections of code throughout the script begin with *if* statements that evaluate the *processdata* flag, and only execute their code segment if required. At the start of the script I include comments that specify what each level of the flag will do, and usually estimates of the time and storage space required, so that the user can make an informed decision about what they want to do.

## Downloading data and other resources

R has a native function called *download.file* that can be used to copy files from the internet to the local computer. This is fine if you have a small number of files to download and they all have traditional static URLs that are unlikely to change. However most of the time we are more likely to store files in a repository such as the Open Science Framework site. The [*osfr*](https://cran.r-project.org/package=osfr) package provides some useful tools for uploading, downloading, and also indexing OSF repositories. All you need to know is the five character identifier for the repository you are interested in. For example, for the repository [https://osf.io/kthg3/](https://osf.io/kthg3/) the five character code is the last part of the URL: kthg3

The first thing we need to do is index the root repository and see what files it contains:

```{r repositoryindexing, include=TRUE}

library(osfr)

nodeID <- osf_retrieve_node('kthg3')
filelist <- osf_ls_files(nodeID, n_max=Inf)

filelist

```

We can then download any of the files we might need, for example this line of code will donwload the first file in the list:

```{r OSFdownloading, include=TRUE, eval=FALSE}

osf_download(filelist[1,])

```

The main thing we are likely to download is data for analysis, but we could also download additional scripts, images, or anything else required for running the analysis. You can specify the local directory where you want to store the file (more on that in the next section). I usually use text matching on the file names to identify the files I need, e.g.:

```{r OSfdownloading2, include=TRUE, eval=FALSE}

id <- pmatch('2011datalong.csv',filelist$name)
# only download the file if it doesn't already exist
if (!file.exists('local/2011datalong.csv')){
osf_download(filelist[id,],'local/')
}

```

Note that we should avoid repeatedly downloading files that are already present locally, for example by using an *if* statement as above. There are also functions in the *osfr* package to enable automated uploading of files, which I find more stable than the drag and drop web interface. In order to upload files (or download them from private repositories) you need to provide an authentication key. However it is possible to download files from any public repository without requiring any log in details or authentication (as above), so our markdown files should work on any machine with internet access. There is also a Python package called [osfclient](https://osfclient.readthedocs.io/en/latest/index.html) that works in a similar way.

## Local file management

When we download files, we need to store them somewhere sensible. I like having a directory in the project folder called something like */local* or */temp*, and then organising this into sensible subdirectories, such as */rawdata* and */processeddata*. Below we will introduce Github, which has quite strict storage limitations. Rather than syncing these (potentially very large) directories of local files to Github, we can add the path to a file called .gitignore, which means that *git* will not try to sync it. Other subdirectories might also be useful, including */Figures* and */scripts*, depending on the files that are required to run the analysis. Note that it is generally a bad idea to use absolute referencing for file locations, for example including a path such as `C:\user\daniel\Documents` will not work on all devices. During the ReproduceMe project, we will check that scripts work on multiple operating systems (Mac, Windows, Linux) to avoid any obvious problems.

## Automating figure generation

One of my favourite things about R is that it can produce really excellent publication quality figures. I always found Matlab a bit lacking in this respect, and before switching to R I used to use a truly painful package called [Grace](https://plasma-gate.weizmann.ac.il/Grace/) (it can get good results, it's just very laborious). It is possible to generate figures directly in code chunks, and have them appear in your markdown file. This is the approach I have used in Figure \@ref(fig:repropyramid) of this document (see the source Markdown code for details). However it is usually the case that journals want figures uploaded in separate files, and saving figures to an external file gives us more control over the exact sizing and other parameters of the plot.

With this in mind, my advice would be to use the *pdf* function when plotting figures, so that the output is saved to a pdf file (related functions such as *ps* and *tiff* can be used in the same way if other file formats are required). We open a new pdf file like this:

```{r outputtopdf, include=TRUE, eval=FALSE}
pdf('Figures/myfigure.pdf', bg="transparent", height = 8, width = 8)
```

Then we call whatever plotting functions we want to to generate our figure. I tend to do this using base R plotting functions, though many people use the *ggplot2* library. There are methods for splitting a figure up into multiple sub-panels, such as by using the *par* function. For example:

```{r plotgridsetup, include=TRUE, eval=FALSE}
par(mfcol=c(1,3))
```

specifies a 1 row by 3 column plot layout. New plots appear in successive locations within this grid. Once we are finished plotting, we close the file with:

```{r closefigure, include=TRUE, eval=FALSE}
dev.off()
```

Finally, at the appropriate place in the text we can include our figure:

```{r inserfigure, include=TRUE, eval=FALSE}

knitr::include_graphics('Figures/myfigure.pdf')

```

Notice how this might interact with the *processdata* flag we talked about above. At the lowest level (*processdata* = 0) the figures won't get generated at all, but since they exist as independent files they can still be loaded in when creating the manuscript.

For really complicated plots with irregular layouts, it's sometimes necessary to do things in a slightly different way, e.g. by combining multiple postscript files and raster images at arbitrary locations. I explain how to do this in the plotting chapter (Ch 18) of my R book, which is available at: [https://eprints.whiterose.ac.uk/181926/](https://eprints.whiterose.ac.uk/181926/). And often in papers we need to generate a diagram in a different piece of software, like Adobe Illustrator. It's totally fine to import things like this as graphics, rather than generate them automatically.

## Python chunks

It is possible to include sections of Python code in an R markdown document. This might be useful if one needs to access Python libraries and toolboxes. It requires the [*reticulate*](https://CRAN.R-project.org/package=reticulate) package to be available, through which it is possible to specify which Python version you wish to use on your machine (it must already have been installed). A python code chunk resembles an R code chunk, and is initiated by three reverse ticks followed by `{python}` (and terminated with three more reverse ticks). Within a python code chunk it is possible to import packages from the local python instance in the usual way, and conduct any Python analyses.

The Python instance has a sandboxed area of memory that cannot directly access the variables in the R Environment. However a special R variable is created called *py* that stores any Python variables as subfields. So a variable called *pyvar* in the Python code can be accessed in R as `py$pyvar`. Similarly, creating a new field of the *py* variable, such as `py$Rdata` will cause a variable called *Rdata* to appear in the Python memory. In this way it is possible to pass information between R and Python. It is a little more clunky than having a shared memory, but presumably necessary for technical reasons (i.e. to do with different data types). It is possible to do something similar for other languages, including Julia, C++, JavaScript, bash and SQL. I haven't tried this, but apparently it's also possible to use Octave in this way, which is an open source Matlab alternative. If we end up with some Matlab code that we can't translate to R, this might be a useful option.

## Calling Matlab and other functions from the command line

Frustratingly, it is not possible to directly access the Matlab kernel through Markdown in the way we can with Python. There is a workaround, which is to create a Matlab script and send this directly to Matlab via the terminal. Annoyingly there is no way to directly store the outputs in R's memory (though they do get echoed to the console if you omit the semicolons at the end of each line), so the script will need to save any outputs to an external file, which would then be read back in by R. This is not ideal, but it does enable access to Matlab-only toolboxes that might be critical for some analyses. It is also necessary to tell the terminal exactly where Matlab exists on the machine, which will probably need to be altered by the user. Here is some example code that generates a simple Matlab script, and then executes it via the terminal:

```{r matlabscripting, include=TRUE, eval=FALSE}

# build the matlab script
line1 <- "a = 1:3:30;"
line2 <- "output = a.^2;"
line3 <- "save('matlaboutput.mat','output')"
matlab_lines <- c(line1, line2, line3)

# output the script to an external file
writeLines(matlab_lines, con='~/myscript.m')

# execute the script in a Matlab instance with no GUI, called through the terminal
system("/Applications/MATLAB_R2022b.app/bin/matlab -nodisplay -r \"run('~/myscript.m'); exit\"")

# use the R.matlab library to read the data file into R
library(R.matlab)
m <- readMat('~/matlaboutput.mat')

# store the output variable in a native R vector
output <- m$output

# clean up by deleting the script and data file
file.remove('~/myscript.m')
file.remove('~/matlaboutput.mat')

```

It should be possible to use a similar approach to call other programs that are accessible directly from the terminal, for example FSL, Freesurfer, and other neuroimaging software.

## Output formats

There are a wide array of possible output formats available when using Markdown. The most useful are pdf, HTML and Microsoft Word. I include the following code in the header information of every Markdown file. RStudio then makes all three file formats available in the dropdown menu next to the 'Knit' button at the top of the script.

```{r outputformats, include=TRUE, eval=FALSE}

bookdown::pdf_document2:
  fig_caption: yes
  toc: no
  keep_tex: yes
word_document: default
html_document: default

```
  
Note that *pdf_document2* from the *bookdown* package seems to work much better than the default *pdf_document*, which can't cope with figure numbering for some reason. Actually Word and HTML documents also don't seem to handle figure referencing properly either, but we need them sometimes.

## Typesetting equations using LaTeX

When Markdown creates pdf files as output, it actually uses [\LaTeX{}](https://www.latex-project.org/) (pronounced 'laytech', or sometimes 'laatech'). This is a document typesetting system very popular in the mathematical sciences because it is good at typesetting equations, and it is possible to incorporate pieces of \LaTeX{} code into a Markdown file (such as the fancy text in this paragraph for the word Latex). Some journals also accept \LaTeX{} files for submission. A full overview of how to use LaTeX is beyond the scope of this document. However it is possible to incorporate equations and mathematical symbols between pairs of dollar signs For example, the code `$y = \sqrt{x^2}$` generates the equation: $y = \sqrt{x^2}$.

## Citations and automatic figure and equation referencing

We can include citations using a BibTex file containing the references. This can be exported from most reference management software (e.g. [Zotero](https://www.zotero.org/)), or created in a native BibTex package such as [BibDesk](https://bibdesk.sourceforge.io/). We specify the reference file in the header information of the markdown file:

```{r referencefile, include=TRUE, eval=FALSE}

bibliography: references.bib

```

Then, we can cite a paper either inline as `@Peikert2021`, which renders as @Peikert2021, or in square brackets as `[@Peikert2021]`, which renders as [@Peikert2021]. The bibliography appears at the end of the pdf file (see below). We can set the style of the bibliography using a csl style file. The eLife style is used for this document; it needs to be in the project directory, and is specified at the top of the markdown script:

```{r stylefile, include=TRUE, eval=FALSE}

csl: elife.csl

```

Similarly we can reference figures by giving them a tag. For example the pyramid figure has the tag *repropyramid*, so we can reference it with `\@ref(fig:repropyramid)`, which renders as Figure \@ref(fig:repropyramid). Equations work similarly but with an independent number sequence from the figures, indexed by the *eqn:* tag.

## Github syncing

Github is a useful tool for version management and online code storage. It integrates well with both OSF and RStudio. If we set up a new public repository on the Github website, we can pull it down to our local computer in RStudio by choosing File: New Project. Select Version Control and then Git. It turns out that Github changed the way to interface with repositories, so much of the information online is out of date. The correct format for the repository URL is: `git@github.com:bakerdh/ReproduceMe.git`

The files will download, and we can then make modifications and synchronise back to the repository by Committing and then Pushing in the Git tab (upper right pane in RStudio). There's a lot more that Github can do, but frankly I don't really understand it, and it's beyond what we'll need for this project.

## Package management

The package management system in R is excellent, in that most of what we might need is available from a single repository ([CRAN](https://cran.r-project.org/)), and is stored consistently in a common library on the host computer. However package installation and activation is a little cumbersome. The *install.packages()* function will force install packages even if they are already present on the computer, which wastes a lot of time if it happens each time the script is run. I use the following code to check which required packages are present, install those that are missing, and activate everything:

```{r packageactivation, include=TRUE, eval=FALSE}

packagelist <- c('knitr','remotes','tictoc','R.matlab') # list of CRAN packages

# find which packages are missing and install them
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}

# then activate all the packages with the library function
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))

```

However the above code will install the most recent package versions. Over time, packages get updated (along with the R language itself), and there is a risk that code will develop bugs as a consequence. An alternative approach is the [*renv*](https://rstudio.github.io/renv/articles/renv.html) package, which takes a snapshot of the precise package versions that are being used. This requires some minimal lines of code.

1. Initialise an *renv* 'lock' file to store package information in:

```{r renv1, include=TRUE, eval=FALSE}

renv::init()

```

2. Once you have activated all the packages required for the project, take a snapshot and save it to the lock file:

```{r renv2, include=TRUE, eval=FALSE}

renv::snapshot()

```

3. The above lines of code only need to be called when we first set up the project, after which they could be commented out in the script. If the lock file is included as part of the project, the environment can then be restored with:

```{r renv3, include=TRUE, eval=FALSE}

renv::restore()

```

## Docker containers

The *renv* solution isn't perfect, for example it doesn't control the R version being used. A more comprehensive solution is to 'containerise' the entire analysis environment, including the version of R and all packages required, and any other software (i.e. \LaTeX{}, ghostscript etc.). The most popular solution for this is [*Docker*](https://www.docker.com/), a system that permits specification of a sandboxed Linux kernel on which to build one's analysis. There are pre-built images that contain RStudio available from the [*Rocker*](https://rocker-project.org/) project, which makes some of the initial setup straightforward.

To use Docker, we first need to download the Docker desktop client and install it. Note that this is not possible on some systems (such as at YNiC) because of access restrictions, and so I think that providing a Docker image should be in addition to the other things that we do (e.g. using *renv*), and not the only route to facilitate reproducibility. Next we generate a Dockerfile, which is a plaintext specification of how to build the image - in other words it instructs Docker about what software it needs to download. Here is an example Dockerfile (that can be downloaded from [Github](https://github.com/bakerdh/SizeAdaptation/blob/d3c12cf0a7aa9e9ac82013c30c8b254344946c0e/docker/Dockerfile)):

```{bash, eval = FALSE}

FROM rocker/rstudio:4.2.2

RUN apt-get update -qq
RUN apt-get -y install cmake
RUN apt-get -y --no-install-recommends install libglu1-mesa-dev
RUN apt-get -y install libxml2-dev
RUN apt-get -y install ghostscript
RUN apt-get -y install libxt-dev
RUN apt-get -y install git

RUN R -e 'install.packages("remotes"); remotes::install_version("car", version="3.1-1"); 
remotes::install_version("pracma", version="2.4.2"); 
remotes::install_version("pals", version="1.7"); 
remotes::install_version("grImport", version="0.9-5");
remotes::install_version("tictoc", version="1.1");
remotes::install_version("BayesFactor", version="0.9.12-4.4");
remotes::install_version("ez", version="4.4-0"); 
remotes::install_version("renv", version="0.17.3");
remotes::install_version("bookdown", version="0.32");
remotes::install_version("knitr", version="1.42");
remotes::install_version("markdown", version="1.5");
remotes::install_version("tinytex", version="0.44"); 
remotes::install_version("osfr", version="0.2.9"); 
tinytex::install_tinytex()'


```

The Dockerfile definition has three sections. The first line specifies the base image from the Rocker project - this is a specific version of their RStudio build (4.2.2). There are also builds with common packages pre-installed, such as rocker/tidyverse that will be useful for some projects. The second section specifies some non-R software packages that we might need, including *git* and *ghostscript*. These are downloaded using the *apt-get* Linux command. Finally there is a list of *R* packages to pre-install. Note that this is achieved using the *remotes* package to specify a particular version of each package - the idea here is to 'freeze' the environment so that subsequent package updates don't change the outcome. There is not much about this in the documentation on Docker/Rocker, as most people use these tools for other purposes. For example, the standard way to install R packages would be something like:

```{bash, eval = FALSE}

RUN install2.r car knitr BayesFactor tinytex osfr

```

but this will download the latest versions of each package, which are not guaranteed to always work.

Once the Dockerfile has been created, we build it using a terminal command, such as:

```{bash, eval = FALSE}

docker build -t sizebuild -f docker/Dockerfile .

```

where *sizebuild* is an arbitrary name we want to give the image, and *docker/Dockerfile* is the path to the file on your computer. Running this will usually take some time the first time it is executed on a new system, as there is a lot of code to download, however most of the packages will be cached locally making subsequent executions faster. The end result will be an image called 'sizebuild' that you can see within the Docker Desktop program.

To actually use this image, we need to run it to create a 'container', basically an active instance of the image that we can load data files into. I had originally hoped we might be able to preload data and code into an image, but this does not seem to be particularly practical because the internal volumes are wiped when the container gets created. We make the container by clicking the 'play' icon in Docker Desktop and specifying some options:

- A container name, which can be anything sensible
- A host port, which should generally be 8787
- An Environment variable called PASSWORD with some arbitrary password value

The container then gets generated and appears in the Containers section of Docker Desktop. You can launch it by clicking on the host port, which should be 8787:8787, and it will connect to the RStudio instance in your browser. You will need to enter a username, which is 'rstudio' by default, and the password you chose earlier.

From this point on, everything happens within the familiar RStudio environment, which should have all the specified packages preinstalled. To get scripts and data into the container, the easiest thing is to clone an existing (public) Github repository, by using the Terminal pane of Rstudio (next to the Console tab) and entering something like:

```{bash, eval = FALSE}

git clone https://github.com/bakerdh/SizeAdaptation.git

```

This will download the repository and you can run the code. This integrates well with our methods for automatically downloading code and other files from OSF - they will all be stored locally in the container. However if the container is deleted they will be lost and have to be downloaded again.

Note that at the time of writing there is a slight hurdle for anyone using a recent Mac with an Apple Silicon processor (i.e. the M1 and M2 chips). These use an ARM architecture, for which frozen Rocker builds are not currently available for specific R versions. However it is possible to use a daily build by changing the first line of the Dockerfile to:

```{bash, eval = FALSE}

FROM rocker/rstudio:latest-daily

```

This provides the latest RStudio build, and so it will change over time, but hopefully they will start providing frozen builds for this processor architecture soon. There is an emulation mode, but it doesn't seem to work properly for some reason - the image builds fine, but you can't actually log in to RStudio.

Finally, there are a few systems around that try to do the same sort of thing remotely on a server. This avoids issues with processor architecture. I had a play with [mybinder.org](https://mybinder.org) - this was super easy to set up, but then very slow to build, and couldn't load all of the packages I needed for a proper analysis. It might be useful for some projects though, and actually I successfully 'binderised' this document at: [https://mybinder.org/v2/gh/bakerdh/ReproduceMe/HEAD](https://mybinder.org/v2/gh/bakerdh/ReproduceMe/HEAD)

## Modality-specific issues

There are some analyses that are not amenable to computational reproducibility. In particular, analyses that involve a manual component, such as coding videos, or in MRI analysis identifying fiducial points and drawing retinotopic regions of interest, often cannot be automated. We need to be flexible and creative with these sorts of situations, and think about how best to integrate the manual components into a reproducible pipeline. This might involve creating a file to store the manually coded values which is then loaded in for the rest of the analysis.

# An example

For this example we will download and analyse a data file from a recent study [@Meese2023]. We will run an ANOVA, report the results, and generate a figure. This is a fairly minimal example, but it should scale up to other types of data.

## Download the data

First let's download the data file. It's actually the one we encountered earlier, the spreadsheet .csv file. So here is the code that finds and downloads the file:

```{r datadownload, include=TRUE}

# check if a local directory exists, if not create it
if (!dir.exists('local/')){dir.create('local')}

# only download the file if it doesn't already exist
if (!file.exists('local/2011datalong.csv')){

# check if we have the osfr package:
packagelist <- c('osfr') # list of CRAN packages
# find which packages are missing and install them
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}
# then activate all the packages with the library function
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))

# now index the repository where the file lives
nodeID <- osf_retrieve_node('kthg3')
filelist <- osf_ls_files(nodeID, n_max=Inf)

# find the file in the list of files from the repository
id <- pmatch('2011datalong.csv',filelist$name)

# download to the local directory
osf_download(filelist[id,],'local/')
}

# finally load in the data
data <- read.csv('local/2011datalong.csv')

# show a summary of the data
head(data)

```

## Run the analysis

The data from this study are judgements of the perceived size of circles, before and after adaptation. We will do some basic preprocessing, in which we extract data for a particular condition, pool over repetition, and calculate the adaptation effect for each participant at each level of the independent variable. Then we'll run an ANOVA using a subset of the data. This is reproducing the analysis of the data in Figure 3a of the Meese paper. Note that understanding exactly what the code is doing is not critical as this will be different for every study we look at. The important thing is seeing the process by which we extract the results of the analysis and embed them in the text.

```{r sizeanalysis, include=TRUE, message=FALSE, warning=FALSE}

# extract information about stimulus levels
proportions <- unique(data[,3])
propdB <- 20*log10(proportions)

# convert some variables to factors for use in ANOVA
data[,1] <- as.factor(data[,1])
data[,3] <- as.factor(data[,3])

# average across repetition, converting to 6-D array of condition means
# dimensions are participant, adapt condition, proportion, DV, IV, Basesize
meansettingsdB <- by(20*log10(data$Setting),list(data$Participant,data$Adaptor,
      data$Proportion,data$DV,data$IV,data$BaseSize),mean)

# reshape to run ANOVA
counter <- 0
sizesetting <- NULL
sizeval <- NULL
subj <- NULL
lev <- NULL
for (size in 1:2){
  for (s in 1:8){
    for (level in 1:7){
      counter <- counter + 1
      sizesetting[counter] <- meansettingsdB[s,3,level,2,2,size]-meansettingsdB[s,1,level,2,2,size]
      sizeval[counter] <- size
      subj[counter] <- s
      lev[counter] <- level
    }
  }
}

# check we have the ez and BayesFactor packages:
packagelist <- c('ez','BayesFactor') # list of CRAN packages
# find which packages are missing and install them
missingpackages <- packagelist[!packagelist %in% installed.packages()[,1]]
if (length(missingpackages)>0){install.packages(missingpackages)}
# then activate all the packages with the library function
toinstall <- packagelist[which(!packagelist %in% (.packages()))]
invisible(lapply(toinstall,library,character.only=TRUE))

# run ANOVA for size judgements and size adaptation
aovdata <- data.frame(as.factor(subj),as.factor(lev),as.factor(sizeval),sizesetting)
colnames(aovdata) <- c('subj','lev','sizeval','sizesetting')
anovaoutput <- ezANOVA(data=aovdata,dv=sizesetting,wid=subj,within=list(lev,sizeval),
                       type=3,detailed=TRUE)
# run Bayesian ANOVA on the same data for Bayes factors
aovBF <- anovaBF(sizesetting ~ lev*sizeval + subj, whichModels='all',
                 whichRandom = 'subj', data=aovdata)
allBFs <- extractBF(aovBF)
GGvals <- as.numeric(unlist(anovaoutput[3]))
interactionbf <- log10(allBFs$bf[7] / allBFs$bf[4])

```
## Report the results

The results of the ANOVA are stored in the variable *anovaoutput*, and the Bayes factors are in the variable *allBFs* (and *interactionbf*). We can index these to report the results as follows:

A factorial (7 stimulus size $\times$ 2 adaptor size) repeated measures ANOVA indicated that the main effect of relative stimulus size was significant (F(`r round(anovaoutput$ANOVA$DFn[2]*GGvals[3],digits=1)`,`r round(anovaoutput$ANOVA$DFd[2]*GGvals[3],digits=1)`) = `r round(anovaoutput$ANOVA$F[2],digits=2)`, p < 0.0001, $\eta^2_g$ = `r round(anovaoutput$ANOVA$ges[2],digits=2)`, $log_{10}BF_{10}$ = `r round(log10(allBFs$bf[1]),digits=1)`), but there was no effect of absolute adaptor size (F(`r anovaoutput$ANOVA$DFn[3]`,`r anovaoutput$ANOVA$DFd[3]`) = `r round(anovaoutput$ANOVA$F[3],digits=3)`, p = `r round(anovaoutput$ANOVA$p[3],digits=2)`, $\eta^2_g$ = `r round(anovaoutput$ANOVA$ges[3],digits=2)`, $log_{10}BF_{10}$ = `r round(log10(allBFs$bf[2]),digits=1)`), nor any interaction (F(`r anovaoutput$ANOVA$DFn[4]`,`r anovaoutput$ANOVA$DFd[4]`) = `r round(anovaoutput$ANOVA$F[4],digits=2)`, p = `r round(anovaoutput$ANOVA$p[4],digits=2)`, $\eta^2_g$ = `r round(anovaoutput$ANOVA$ges[4],digits=2)`, $log_{10}BF_{10}$ = `r round(interactionbf,digits=1)`).

You can check the markdown file to see how I have extracted the results of the statistical tests, but it is all done with in-line R snippets, and judicious use of the *round* function. This is the critical thing - we don't manually type in the values from the ANOVA, they are automatically inserted into the text!

## Plot a figure

Finally we can create a figure to summarise the results as follows:

```{r makefigure, include=TRUE, message=FALSE, warning=FALSE}

if (!dir.exists('Figures/')){dir.create('Figures')}

pdf('Figures/datafigure.pdf', bg="transparent", height = 5, width = 5)

colpal <- c("#FBB4AE", "#B3CDE3", "#CCEBC5", "#DECBE4", "#FED9A6", "#FFFFCC", "#E5D8BD", "#FDDAEC", "#F2F2F2")

plotlims <- c(-12,12,-3,3) 
ticklocsx <- seq(-12,12,6)   
ticklocsy <- 20*log10(seq(70,140,10)/100)   
ticklabelsx <- c("25","50","100","200","400")      
ticklabelsy <- seq(-30,40,10)   

par(pty="s") 
plot(x=NULL,y=NULL,axes=FALSE, ann=FALSE, xlim=plotlims[1:2], ylim=plotlims[3:4])   
axis(1, at=ticklocsx, tck=0.01, lab=F, lwd=2)    
axis(2, at=ticklocsy, tck=0.01, lab=F, lwd=2)
mtext(text = ticklabelsx, side = 1, at=ticklocsx)   
mtext(text = ticklabelsy, side = 2, at=ticklocsy, line=0.2, las=1)
title(xlab="Target size (% of adaptor)", col.lab=rgb(0,0,0), line=1.2, cex.lab=1.5)   
title(ylab="Size shift (%)", col.lab=rgb(0,0,0), line=1.5, cex.lab=1.5)

lines(c(-12,12),c(0,0),lty=2,lwd=2)

datasubset <- meansettingsdB[,3,,2,2,1]-meansettingsdB[,1,,2,2,1]
toplot <- apply(datasubset,2,mean)
toplotSE <- apply(datasubset,2,sd)/sqrt(8)
arrows(propdB,toplot,x1=propdB, y1=toplot-toplotSE, length=0.015, angle=90, lwd=2)  
arrows(propdB,toplot,x1=propdB, y1=toplot+toplotSE, length=0.015, angle=90, lwd=2)  
lines(propdB,toplot,lwd=3,col=colpal[1])
points(propdB,toplot,pch=21,cex=2,bg=colpal[1],lwd=2)

datasubset <- meansettingsdB[,3,,2,2,2]-meansettingsdB[,1,,2,2,2]
toplot <- apply(datasubset,2,mean)
toplotSE <- apply(datasubset,2,sd)/sqrt(8)
arrows(propdB,toplot,x1=propdB, y1=toplot-toplotSE, length=0.015, angle=90, lwd=2)  
arrows(propdB,toplot,x1=propdB, y1=toplot+toplotSE, length=0.015, angle=90, lwd=2)  
lines(propdB,toplot,lwd=3,col=colpal[2])
points(propdB,toplot,pch=22,cex=2,bg=colpal[2],lwd=2)

text(-3,2.5,'Aligned adaptor',cex=1.2,pos=4)
legend(-12, 20*log10(1.4), c("1 deg","2 deg"), title='Adaptor size', 
       cex=1, pt.cex=2, pt.bg=colpal[1:2], pch=21:22, pt.lwd=2, box.lwd=2)

invisible(dev.off()) # the invisible function prevents reporting success

```

Notice that we have generated the figure but not displayed it yet. We can then load in and display the figure in the output:

```{r datafig, fig.cap="Summary of size adaptation data from Meese and Baker (2023).", fig.align="center"}

knitr::include_graphics('Figures/datafigure.pdf')

```

The figure appears in the output document as Figure \@ref(fig:datafig). Where possible I guess we'll try and produce exactly the same figures as in the original manuscript that is provided by the authors (this will be much easier if they've used R to do the plotting!).

# References
